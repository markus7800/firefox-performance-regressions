{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.modeleval_utils import *\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no feature selection\n",
    "#output_dir = 'experiments/results'\n",
    "#drop_columns = False # feature selection\n",
    "\n",
    "# feature selection\n",
    "output_dir = 'experiments/results_FS'\n",
    "drop_columns = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training all models with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(output_dir, model, target, data, feature_type, scoring, X_train, y_train, X_test, y_test):\n",
    "    evaluation = {'model': model}\n",
    "\n",
    "    pipeline, best_params, best_result = get_pipeline(output_dir, data, feature_type, target, scoring, model)\n",
    "\n",
    "    y_test_proportion = len(y_test) / (len(y_train) + len(y_test))\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=round(len(y_train) * y_test_proportion))\n",
    "    res = cross_validate(pipeline, X_train, y_train, scoring=['average_precision', 'roc_auc'], cv=tscv, n_jobs=5)\n",
    "    evaluation['validation'] = res\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    evaluation['fitted_pipeline'] = pipeline\n",
    "    evaluation['best_params'] = best_params\n",
    "    evaluation['best_result'] = best_result\n",
    "\n",
    "    threshold_train, f1_train = get_best_f1_threshold(pipeline, X_train, y_train)\n",
    "    threshold_test, f1_test = get_best_f1_threshold(pipeline, X_test, y_test)\n",
    "    print(f'{threshold_train=} {f1_train=}, {threshold_test=}, {f1_test=}')\n",
    "\n",
    "    for (split, X_, y_, threshold) in [\n",
    "        ('train', X_train, y_train, threshold_train),\n",
    "        ('test', X_test, y_test, threshold_train),\n",
    "        ('test_pareto', X_test, y_test, threshold_test)\n",
    "        ]:\n",
    "\n",
    "        y_score = get_y_score(pipeline, X_)\n",
    "        y_pred = y_score >= threshold\n",
    "\n",
    "        evaluation[split] = {\n",
    "            'average_precision': metrics.average_precision_score(y_, y_score),\n",
    "            'roc_auc': metrics.roc_auc_score(y_, y_score),\n",
    "\n",
    "            'threshold': threshold,\n",
    "            \n",
    "            'regression': {\n",
    "                'recall': metrics.recall_score(y_, y_pred, pos_label=1),\n",
    "                'precision': metrics.precision_score(y_, y_pred, pos_label=1, zero_division=0),\n",
    "                'f1': metrics.f1_score(y_, y_pred, pos_label=1)\n",
    "            },\n",
    "            'non-regression': {\n",
    "                'recall': metrics.recall_score(y_, y_pred, pos_label=0),\n",
    "                'precision': metrics.precision_score(y_, y_pred, pos_label=0, zero_division=0),\n",
    "                'f1': metrics.f1_score(y_, y_pred, pos_label=0)\n",
    "            },\n",
    "            'avg_weighted': {\n",
    "                'recall': metrics.recall_score(y_, y_pred, average='weighted'),\n",
    "                'precision': metrics.precision_score(y_, y_pred, average='weighted', zero_division=0),\n",
    "                'f1': metrics.f1_score(y_, y_pred, average='weighted')\n",
    "            },\n",
    "            'avg_macro': {\n",
    "                'recall': metrics.recall_score(y_, y_pred, average='macro'),\n",
    "                'precision': metrics.precision_score(y_, y_pred, average='macro', zero_division=0),\n",
    "                'f1': metrics.f1_score(y_, y_pred, average='macro')\n",
    "            }\n",
    "        }\n",
    "\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "def evaluate_pipelines(output_dir, target, data, feature_type, scoring, X_train, y_train, X_test, y_test):\n",
    "    evaluations = []\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        try:\n",
    "            evaluation = evaluate_pipeline(output_dir, model, target, data, feature_type, scoring, X_train, y_train, X_test, y_test)\n",
    "            evaluations.append(evaluation)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_by_config = {}\n",
    "for (data, target) in [('fixed_defect_szz', 'performance'), ('bugbug_buglevel', 'performance'), ('bugbug_buglevel', 'regression')]:\n",
    "    for feature_type in ['traditional', 'bow']:\n",
    "        print('\\n\\n', '--> ', data, target, feature_type, '\\n\\n')\n",
    "        scoring = 'average_precision'\n",
    "\n",
    "        X, y, features = data_map[feature_type][data](target, drop_columns)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "        evaluations = evaluate_pipelines(output_dir, target, data, feature_type, scoring, X_train, y_train, X_test, y_test)\n",
    "        evaluations_by_config[f'{data}_{target}_{feature_type}'] = evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(output_dir, 'evaluations.pickle'), 'wb') as f:\n",
    "    pickle.dump(evaluations_by_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables with Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir, drop_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with  open(os.path.join(output_dir, 'evaluations.pickle'), 'rb') as f:\n",
    "    evaluations_by_config = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparam_search_stat_df(evaluation):\n",
    "    columns = [\n",
    "        np.array(['train', 'train', 'validation', 'validation', 'test', 'test']),\n",
    "        np.array(['Avg. Precision', 'AUC', 'Avg. Precision', 'AUC', 'Avg. Precision', 'AUC']),\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame([[\n",
    "        evaluation['train']['average_precision'],\n",
    "        evaluation['train']['roc_auc'],\n",
    "        evaluation['validation']['test_average_precision'].mean(),\n",
    "        evaluation['validation']['test_roc_auc'].mean(),\n",
    "        evaluation['test']['average_precision'],\n",
    "        evaluation['test']['roc_auc']\n",
    "        ]],\n",
    "        index=[model_names[evaluation['model']]],\n",
    "        columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report_df(evaluation):\n",
    "    dfs = []\n",
    "    for split in ['train', 'test', 'test_pareto']:\n",
    "        df = pd.DataFrame(\n",
    "            [\n",
    "                evaluation[split]['regression'],\n",
    "                evaluation[split]['non-regression']\n",
    "                # evaluation[split]['avg_macro'],\n",
    "                # evaluation[split]['avg_weighted']\n",
    "            ],\n",
    "            index=[\n",
    "                [model_names[evaluation['model']]]*2,\n",
    "                ['regression', 'non-regression']#, 'weighted average', 'micro average']\n",
    "                ]\n",
    "        )\n",
    "        df.columns = [\n",
    "            np.array([split]*3),\n",
    "            np.array(['recall', 'precision', 'F1'])\n",
    "        ]\n",
    "        dfs.append(df)\n",
    "\n",
    "    classification_reports = pd.concat(dfs, axis=1)\n",
    "\n",
    "    return classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(evaluations_by_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'bugbug_buglevel_regression_bow'\n",
    "evaluations = evaluations_by_config[config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = config.split('_')\n",
    "feature_type = ws.pop()\n",
    "target = ws.pop()\n",
    "data = '_'.join(ws)\n",
    "data, target, feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat(\n",
    "    [get_hyperparam_search_stat_df(evaluation) for evaluation in evaluations],\n",
    "     axis=0)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in stats.columns: #[('test', 'Avg. Precision'), ('test', 'AUC')]:\n",
    "    amax = stats.loc[:, c].argmax()\n",
    "    stats.loc[stats.index[amax], c] = f'\\\\textbf{{{stats.iloc[amax][c]:.4f}}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = pd.concat(\n",
    "    [get_classification_report_df(evaluation) for evaluation in evaluations],\n",
    "     axis=0)\n",
    "classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [('train', 'F1'), ('test', 'F1'), ('test_pareto', 'F1')]:\n",
    "    reg = classification_reports.loc[pd.IndexSlice[:,'regression', :]]\n",
    "    amax = reg[c].argmax()\n",
    "    v = classification_reports.loc[(reg.index[amax], 'regression'), c]\n",
    "    classification_reports.loc[(reg.index[amax], 'regression'), c] = f'\\\\textbf{{{v:.4f}}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = classification_reports.to_latex(escape=False)\n",
    "s = s.replace('Dummy Classifier', '\\\\multirow{2}{2.7cm}{Dummy\\\\\\\\Classifier}')\n",
    "s = s.replace('\\nLogistic Regression', '\\\\hline\\\\hline\\n\\\\multirow{2}{2.7cm}{Logistic\\\\\\\\Regression}')\n",
    "s = s.replace('\\nSupport Vector Machine', '\\\\hline\\\\hline\\n\\\\multirow{2}{2.7cm}{Support Vector\\\\\\\\Machine}')\n",
    "s = s.replace('\\nMulti-Layer Perceptron', '\\\\hline\\\\hline\\n\\\\multirow{2}{2.7cm}{Multi-Layer\\\\\\\\Perceptron}')\n",
    "s = s.replace('\\nRandom Forest', '\\\\hline\\\\hline\\nRandom Forest')\n",
    "s = s.replace('\\nXGBoost ', '\\\\hline\\\\hline\\nXGBoost')\n",
    "s = s.replace('\\nTPOT ', '\\\\hline\\\\hline\\nTPOT')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, features = data_map[feature_type][data](target, drop_columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_pipelines = [(model_names[evaluation['model']], evaluation['fitted_pipeline']) for evaluation in evaluations[1:]] # exclude dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring='average_precision'\n",
    "plot_roc_auc_rec_prec_for_all_models(target, data, feature_type, scoring,\n",
    "    fitted_pipelines, X_train, X_test, y_train, y_test, save=False, figsize=(6,4), ylim=[0, 0.3 if data == 'fixed_defect_szz' else 1.0], output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4696bc64be58b0e5e207e22dca014ef47cd404c337c3ddfbdfdc381921ec8122"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
