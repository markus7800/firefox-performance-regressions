{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.modeleval_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#output_dir = 'experiments/results'\n",
    "output_dir = 'experiments/results_FS' # feature reduction\n",
    "with  open(os.path.join(output_dir, 'evaluations.pickle'), 'rb') as f:\n",
    "    evaluations_by_config = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'average_precision'\n",
    "all_sampler_ranks = []\n",
    "for output_dir in ['experiments/results_FS']:\n",
    "    for (data, target) in [('fixed_defect_szz', 'performance'), ('bugbug_buglevel', 'performance')]: #('bugbug_buglevel', 'regression')\n",
    "        for feature_type in ['traditional', 'bow']:\n",
    "            #if output_dir == 'experiments/results_FS' and feature_type == 'bow':\n",
    "            #    continue\n",
    "            for model in models:\n",
    "                try:\n",
    "                    results = get_results(output_dir, data, feature_type, target, scoring, model)\n",
    "                    results = results.replace({\n",
    "                        None: 'No sampling',\n",
    "                        'RandomOverSampler(random_state=0)': 'Random Over-Sampling',\n",
    "                        'RandomUnderSampler(random_state=0)': 'Random Under-Sampling',\n",
    "                        'SMOTE(random_state=0)': 'SMOTE'\n",
    "                        })\n",
    "                    sampler_ranks = results.groupby('param_sampler')[['mean_test_score']].max().sort_values('mean_test_score', ascending=False)\n",
    "                    sampler_ranks['rank'] = np.arange(1,5)\n",
    "                    sampler_ranks['model'] = np.full(4, model)\n",
    "                    all_sampler_ranks.append(sampler_ranks)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "all_sampler_ranks = pd.concat(all_sampler_ranks)\n",
    "all_sampler_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = all_sampler_ranks.groupby('param_sampler')[['rank']].mean().sort_values('rank', ascending=True)\n",
    "t.index.name = 'Sampling Method'\n",
    "t = t.rename({'rank': 'Average Rank'}, axis=1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'average_precision'\n",
    "best_sampler = []\n",
    "for (data, target) in [('fixed_defect_szz', 'performance'), ('bugbug_buglevel', 'performance'), ('bugbug_buglevel', 'regression')]:\n",
    "    for feature_type in ['traditional', 'bow']:\n",
    "        for model in models:\n",
    "            try:\n",
    "                results = get_results(output_dir, data, feature_type, target, scoring, model)\n",
    "                results = results.replace({\n",
    "                    None: 'No sampling',\n",
    "                    'RandomOverSampler(random_state=0)': 'Over-Sampling',\n",
    "                    'RandomUnderSampler(random_state=0)': 'Under-Sampling',\n",
    "                    'SMOTE(random_state=0)': 'SMOTE'\n",
    "                    })\n",
    "                best_sampler.append({\n",
    "                    'Data': data,\n",
    "                    'Feature Type': feature_type,\n",
    "                    'Target': target,\n",
    "                    'Model': model_names[model],\n",
    "                    'Sampling':results.iloc[0]['param_sampler'],\n",
    "                    'AP (val.)': results.iloc[0]['mean_test_score']\n",
    "                    })\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "best_sampler = pd.DataFrame(best_sampler)\n",
    "best_sampler = best_sampler.replace({\n",
    "    'fixed_defect_szz': 'SZZ',\n",
    "    'bugbug_buglevel': 'bugbug'\n",
    "})\n",
    "best_sampler = best_sampler.replace({\n",
    "    'performance': 'perf. regressions',\n",
    "    'regression': 'all regressions',\n",
    "    'bow': 'bag-of-words'\n",
    "})\n",
    "best_sampler.set_index(pd.MultiIndex.from_frame(best_sampler[['Data', 'Target', 'Feature Type']]), inplace=True)\n",
    "best_sampler = best_sampler.drop(['Data', 'Target', 'Feature Type'], axis=1)\n",
    "best_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_sampler.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sampler = []\n",
    "\n",
    "scoring = 'average_precision'\n",
    "for config, evaluations in evaluations_by_config.items():\n",
    "    ws = config.split('_')\n",
    "    feature_type = ws.pop()\n",
    "    target = ws.pop()\n",
    "    data = '_'.join(ws)\n",
    "\n",
    "    amax = np.argmax([evaluation['test']['regression']['f1'] for evaluation in evaluations])\n",
    "    evaluation = evaluations[amax]\n",
    "\n",
    "    sampler = None\n",
    "    if evaluation['model'] == 'tpot':\n",
    "        sampler = None\n",
    "    elif evaluation['best_params']:\n",
    "        sampler = evaluation['best_params']['sampler']\n",
    "\n",
    "    best_sampler.append({\n",
    "        'Data': data,\n",
    "        'Target': target,\n",
    "        'Feature Type': feature_type,\n",
    "        'Model':  model_names[evaluation['model']],\n",
    "        'recall': evaluation['test']['regression']['recall'],\n",
    "        'precision': evaluation['test']['regression']['precision'],\n",
    "        'F1': evaluation['test']['regression']['f1'],\n",
    "        'Sampling Method': str(sampler)\n",
    "        })\n",
    "\n",
    "best_sampler = pd.DataFrame(best_sampler)\n",
    "best_sampler = best_sampler.replace({\n",
    "                    'None': 'No sampling',\n",
    "                    'RandomOverSampler(random_state=0)': 'Random Over-Sampling',\n",
    "                    'RandomUnderSampler(random_state=0)': 'Random Under-Sampling',\n",
    "                    'SMOTE(random_state=0)': 'SMOTE'\n",
    "                    })\n",
    "\n",
    "best_sampler = best_sampler.replace({\n",
    "    'fixed_defect_szz': 'SZZ',\n",
    "    'bugbug_buglevel': 'bugbug'\n",
    "})\n",
    "best_sampler = best_sampler.replace({\n",
    "    'performance': 'perf. regressions',\n",
    "    'regression': 'all regressions'\n",
    "})\n",
    "\n",
    "best_sampler.set_index(pd.MultiIndex.from_frame(best_sampler[['Data', 'Target', 'Feature Type']]), inplace=True)\n",
    "best_sampler = best_sampler.drop(['Data', 'Target', 'Feature Type'], axis=1)\n",
    "\n",
    "best_sampler    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_sampler.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4696bc64be58b0e5e207e22dca014ef47cd404c337c3ddfbdfdc381921ec8122"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
