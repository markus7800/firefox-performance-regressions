{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.modeleval_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no feature selection\n",
    "#output_dir = 'experiments/results'\n",
    "#drop_columns = False # feature selection\n",
    "\n",
    "# feature selection\n",
    "output_dir = 'experiments/results_FS'\n",
    "drop_columns = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_best_model(model, data, feature_type, target, scoring):\n",
    "    X, y, features = data_map[feature_type][data](target, drop_columns)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "    pipeline, best_params, best_result = get_pipeline(output_dir, data, feature_type, target, scoring, model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    t, f1 = get_best_f1_threshold(pipeline, X_train, y_train)\n",
    "    return pipeline, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_threshold(clf, X, t):\n",
    "    y_score = get_y_score(clf, X)\n",
    "    return (y_score >= t).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {\n",
    "    'szz_traditional': data_map['traditional']['fixed_defect_szz']('performance', drop_columns),\n",
    "    'szz_bow': data_map['bow']['fixed_defect_szz']('performance'),\n",
    "    'bugbug_traditional': data_map['traditional']['bugbug_buglevel']('performance', drop_columns),\n",
    "    'bugbug_bow': data_map['bow']['bugbug_buglevel']('performance'),\n",
    "    'bugbug_reg_traditional': data_map['traditional']['bugbug_buglevel']('regression', drop_columns),\n",
    "    'bugbug_reg_bow': data_map['bow']['bugbug_buglevel']('regression')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'szz_traditional': fit_best_model('lr', 'fixed_defect_szz', 'traditional', 'performance', 'average_precision'),\n",
    "    'szz_bow': fit_best_model('lr', 'fixed_defect_szz', 'bow', 'performance', 'average_precision'),\n",
    "    'bugbug_traditional': fit_best_model('lr', 'bugbug_buglevel', 'traditional', 'performance', 'average_precision'),\n",
    "    'bugbug_bow': fit_best_model('lr', 'bugbug_buglevel', 'bow', 'performance', 'average_precision'),\n",
    "    'bugbug_reg_traditional': fit_best_model('lr', 'bugbug_buglevel', 'traditional', 'regression', 'average_precision'),\n",
    "    'bugbug_reg_bow': fit_best_model('lr', 'bugbug_buglevel', 'bow', 'regression', 'average_precision')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_name_map import feature_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "kind = 'bugbug_traditional'\n",
    "\n",
    "X, y, f = datas[kind]\n",
    "pipeline, t = pipelines[kind]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "clf = pipeline['model']\n",
    "if 'traditional' in kind:\n",
    "    X_ = pipeline['scaler'].transform(X_test)\n",
    "    f = f.drop('target', axis=1)\n",
    "    feature_names = [feature_name_map[c] for c in f.columns]\n",
    "else:\n",
    "    X_ = X_test.toarray()\n",
    "    feature_names = f\n",
    "\n",
    "explainer = shap.Explainer(clf, X_, feature_names=feature_names)\n",
    "print(explainer)\n",
    "shap_values = explainer.shap_values(X_)\n",
    "\n",
    "shap.summary_plot(shap_values, X_, max_display=15, feature_names=feature_names, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'plots_interpretability/summary_{kind}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commitlevel_ix = 90291\n",
    "buglevel_ix = 61674\n",
    "ix = buglevel_ix if 'bugbug' in kind else commitlevel_ix\n",
    "\n",
    "x = X_[[ix - X_train.shape[0]],:]\n",
    "\n",
    "explainer = shap.Explainer(clf, X_, feature_names=feature_names)\n",
    "shap_values = explainer(x)\n",
    "plt.ioff()\n",
    "shap.waterfall_plot(shap_values[0], show=False)\n",
    "#plt.gcf()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'plots_interpretability/waterfall_{kind}.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding candidate commit for investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_buglevel = pd.read_csv('data/feature_extractor/features_buglevel.csv')\n",
    "F_commitlevel = pd.read_csv('data/feature_extractor/features_commitlevel.csv')\n",
    "\n",
    "rev_to_commitlevel_ix = {rev: i for i, rev in enumerate(F_commitlevel['revision'])}\n",
    "commitlevel_ix_to_rev = list(F_commitlevel['revision'])\n",
    "\n",
    "rev_to_buglevel_ix = {}\n",
    "buglevel_ix_to_rev = list(F_buglevel['revisions'])\n",
    "for i, row in F_buglevel[['first_revision', 'revisions']].iterrows():\n",
    "    first_rev = row['first_revision']\n",
    "    revs = row['revisions']\n",
    "    #print(i, first_rev, revs)\n",
    "    for rev in revs.split(','):\n",
    "        rev_to_buglevel_ix[rev] = i\n",
    "\n",
    "del F_buglevel; del F_commitlevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rev_to_commitlevel_ix), len(commitlevel_ix_to_rev), len(rev_to_buglevel_ix), len(buglevel_ix_to_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szz_tps = read_data_from_json('experiments/results/szz_regressed_by_tp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szz_tps_buglevel_ix = []\n",
    "szz_tps_commitlevel_ix = []\n",
    "\n",
    "for rev in szz_tps:\n",
    "    try:\n",
    "        szz_tps_buglevel_ix.append(rev_to_buglevel_ix[rev])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        szz_tps_commitlevel_ix.append(rev_to_commitlevel_ix[rev])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "szz_tps_buglevel_ix = sorted(list(set(szz_tps_buglevel_ix)))\n",
    "szz_tps_commitlevel_ix = sorted(list(set(szz_tps_commitlevel_ix)))\n",
    "len(szz_tps_buglevel_ix), len(szz_tps_commitlevel_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'szz'\n",
    "szz_tps_ixs = szz_tps_commitlevel_ix if kind == 'szz' else szz_tps_buglevel_ix\n",
    "szz_tps_ixs = np.array(szz_tps_ixs)\n",
    "\n",
    "X, y, _ = datas[kind + '_traditional']\n",
    "clf, t = pipelines[kind + '_traditional']\n",
    "y_pred_traditional = predict_at_threshold(clf, X[szz_tps_ixs, :], t)\n",
    "\n",
    "print(y[szz_tps_ixs].sum(), len(szz_tps_ixs))\n",
    "\n",
    "X, y, _ = datas[kind + '_bow']\n",
    "clf, t = pipelines[kind + '_bow']\n",
    "y_pred_bow = predict_at_threshold(clf, X[szz_tps_ixs, :], t)\n",
    "\n",
    "y_pred_traditional.sum(), y_pred_bow.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_threshold = X.shape[0] * 0.9\n",
    "disagreements = szz_tps_ixs[(y_pred_traditional < y_pred_bow)]\n",
    "disagreements[disagreements > test_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_ix = 90291\n",
    "rev = commitlevel_ix_to_rev[commit_ix]\n",
    "bug_ix = rev_to_buglevel_ix[rev]\n",
    "rev, bug_ix, buglevel_ix_to_rev[bug_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'szz'\n",
    "ix = bug_ix if 'bugbug' in kind else commit_ix\n",
    "\n",
    "X, y, _ = datas[kind + '_traditional']\n",
    "clf, t = pipelines[kind + '_traditional']\n",
    "y_pred_traditional = predict_at_threshold(clf, X[[ix], :], t)\n",
    "\n",
    "X, y, _ = datas[kind + '_bow']\n",
    "clf, t = pipelines[kind + '_bow']\n",
    "y_pred_bow = predict_at_threshold(clf, X[[ix], :], t)\n",
    "\n",
    "y_pred_traditional[0], y_pred_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4696bc64be58b0e5e207e22dca014ef47cd404c337c3ddfbdfdc381921ec8122"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
